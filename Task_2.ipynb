{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of task2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au7iOZPtk5pb"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZIENPTPl5QM"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import string \n",
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "xtX1c4ivl7MY",
        "outputId": "6aa948d3-1260-498f-9ad8-102a32e01865"
      },
      "source": [
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "from google.colab import drive\n",
        "\n",
        "# drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-867f6eae-f349-4eef-9f70-bc6a0fd66077\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-867f6eae-f349-4eef-9f70-bc6a0fd66077\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving banglastopword.txt to banglastopword.txt\n",
            "Saving bengali_hatespeech.csv to bengali_hatespeech.csv\n",
            "Saving hindi_hatespeech.tsv to hindi_hatespeech.tsv\n",
            "Saving model to model\n",
            "Saving stopword.txt to stopword.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqAlFhe5m89d"
      },
      "source": [
        "data = pd.read_csv('hindi_hatespeech.tsv', sep='\\t')\n",
        "# data = data.head(100)\n",
        "stop_word_list = pd.read_csv('stopword.txt', sep='\\s+', header=None)\n",
        "stop_word_list = stop_word_list[0].tolist()\n",
        "data['text'] = data['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "data['text'] = data['text'].str.replace('[{}]'.format('।'), '')\n",
        "# data['text'] = data['text'].str.replace('[{}]'.format('\\n'), '')\n",
        "data['text'] = data['text'].str.lower()\n",
        "data['text'] = data['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in (stop_word_list)]))\n",
        "data['task_1'] = data['task_1'].map({'HOF': 1, 'NOT': 0})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kml-YtPPmGEi",
        "outputId": "0bdbb73b-7d7e-4782-92d7-a0bcc95f89df"
      },
      "source": [
        "#TODO: implement!\n",
        "V = list(data['text'].str.split(' ', expand=True).stack().unique())\n",
        "print(len(V))\n",
        "def word_to_one_hot(word, vocab):\n",
        "  return vocab[word]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDfIX195mk6q",
        "outputId": "f9e4728e-4cc7-4d24-8734-067ffc68e8f9"
      },
      "source": [
        "# All words in the corpus\n",
        "def all_word_corpus(data):\n",
        "  all_word_list = []\n",
        "  for value in data.str.split(\" \"):\n",
        "    all_word_list.extend(value)\n",
        "    # for each in value.split(\" \"):\n",
        "    #   # print(each)\n",
        "    # word_list.append(value)\n",
        "  return all_word_list\n",
        "\n",
        "all_word_list = all_word_corpus(data[\"text\"])\n",
        "len(all_word_list)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h22_HiktnmHQ"
      },
      "source": [
        "#TODO: implement!\n",
        "\n",
        "# Subsampling function\n",
        "def sampling_prob(word, corpus):\n",
        "  word_number_list = []\n",
        "  word_freq_dict = {} # Each word frequency dictionary\n",
        "  word_prob_dict = {}\n",
        "  for each in word:\n",
        "    zw_i = corpus.count(each)/ len(corpus)\n",
        "    word_number_list.append(zw_i)\n",
        "    word_freq_dict[each] = zw_i\n",
        "    Pkeepw_i = (math.sqrt(zw_i/0.001) + 1) * (0.001/zw_i)\n",
        "    word_prob_dict[each] = Pkeepw_i\n",
        "  return word_freq_dict, word_prob_dict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pQnHxJjnr-n"
      },
      "source": [
        "word_frequency_dictionary, word_probability_dictionary = sampling_prob(V, all_word_list)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xTm2lg8nvUo"
      },
      "source": [
        "# New unique word list(V), New Word frquency dictionary, new text data(data)\n",
        "def new_word_probability_dictionary(word_probability_dictionary, V, all_word_list, data):\n",
        "  threshold = 1.9\n",
        "  deleted_word_list = []\n",
        "  for i in word_probability_dictionary:\n",
        "    if word_probability_dictionary[i] <= threshold: \n",
        "      deleted_word_list.append(i)\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda x: ' '.join([item for item in x.split() if item not in (deleted_word_list)]))\n",
        "  V = list(data[\"text\"].str.split(' ', expand=True).stack().unique())\n",
        "  all_word_list = all_word_corpus(data[\"text\"])\n",
        "  word_frequency_dictionary, word_probability_dictionary = sampling_prob(V, all_word_list)\n",
        "  return word_frequency_dictionary, word_probability_dictionary, V, data\n",
        "# print(\"word_probability_dictionary:\", len(word_probability_dictionary))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSsvTPJfn-wN",
        "outputId": "055a1cde-44b3-4029-8898-903eaf6e381c"
      },
      "source": [
        "word_frequency_dictionary, word_probability_dictionary, V, data= new_word_probability_dictionary(word_probability_dictionary, V, all_word_list, data)\n",
        "len(word_probability_dictionary)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO1GPuZUoA_E"
      },
      "source": [
        "df = pd.DataFrame(list(zip(V)), columns=['Vocalbulary'])\n",
        "one_hot_vocabulary = pd.get_dummies(df.Vocalbulary)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTyjUr2coKHn",
        "outputId": "bb0a5b86-6d9e-42db-9fc1-45960418f108"
      },
      "source": [
        "for each in range(len(data['text'])):\n",
        "  if len(data['text'][each].split(\" \"))<= 25:\n",
        "    for i in range(25 - len(data['text'][each].split(\" \"))):\n",
        "      data['text'][each] = data['text'][each] + \" 0\"\n",
        "  elif len(data['text'][each].split(\" \"))> 25:\n",
        "    x = data['text'][each].split(\" \")[:25]\n",
        "    data['text'][each] = \" \".join(x)\n",
        "  else:\n",
        "    pass"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU7Dhx9loNOq"
      },
      "source": [
        "# Create model \n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, length, embedding_size):\n",
        "    super().__init__()\n",
        "    # print(\"First.\")\n",
        "    self.input_layer = nn.Linear(in_features=length, out_features=embedding_size, bias=False)\n",
        "    # print(\"Second.\")\n",
        "    self.output = nn.Linear(in_features=embedding_size, out_features=length, bias=False)\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "    x=self.input_layer(one_hot)\n",
        "    x2 = self.output(x)\n",
        "    out = F.log_softmax(x2)\n",
        "    return out\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yD9WoXmoSja",
        "outputId": "72a05ec4-dda8-4c87-d52b-c36735f6b03b"
      },
      "source": [
        "# Previous model load.\n",
        "model = Word2Vec(20150, 300)\n",
        "model.load_state_dict(torch.load(\"model\"))\n",
        "# model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/model\", map_location=torch.device('cpu')))\n",
        "  "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t81vVez6rGE_"
      },
      "source": [
        "# Set hyperparameters\n",
        "window_size = 4\n",
        "embedding_size = 300\n",
        "learning_rate = 0.005\n",
        "epochs = 50"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8icroA_rLXY"
      },
      "source": [
        "class sample_data():\n",
        "\n",
        "    def __init__(self,data):\n",
        "      self.data = data\n",
        "    def __len__(self):\n",
        "       return len(self.data)        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      word_to_one_hot_list_for_tensor = []\n",
        "      for each in data[\"text\"][idx].split(' '):\n",
        "        word_to_one_hot_list_for_tensor.append(word_to_one_hot(each,one_hot_vocabulary))\n",
        "\n",
        "      _label=0.0\n",
        "      if self.data['task_1'][idx]==1:\n",
        "        _label=1.0\n",
        "      return torch.Tensor(word_to_one_hot_list_for_tensor), torch.tensor(_label)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uB9LqUprQ3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81659486-2d57-48bc-b888-5094923d0e6e"
      },
      "source": [
        "sample_custom_dataset = sample_data(data)\n",
        "print(len(sample_custom_dataset))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHGWCLAMsaYf"
      },
      "source": [
        "tr_size = int(0.8 * len(sample_custom_dataset))\n",
        "# print(tr_size)\n",
        "te_size = len(sample_custom_dataset) - tr_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(sample_custom_dataset, [tr_size, te_size])\n",
        "# print(len(train_dataset))\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size= 12, shuffle = True)\n",
        "# print(len(trainloader))\n",
        "testloader = DataLoader(test_dataset, batch_size= 12, shuffle = True)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rvBRJfHraoC"
      },
      "source": [
        "# Create model \n",
        "\n",
        "class Sentiment_Analysis(nn.Module):\n",
        "  def __init__(self, length, embedding_size):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.inputlayer = nn.Linear(in_features=20150*25, out_features=1000, bias=False) \n",
        "    self.bn1 = nn.BatchNorm1d(num_features=1000)\n",
        "\n",
        "    self.nextlayer = nn.Linear(in_features=1000, out_features=300, bias=False)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=300) \n",
        "\n",
        "    self.fc1 = model.input_layer.weight\n",
        "    self.fc2 = model.output.weight\n",
        "\n",
        "    self.pre_out = nn.Linear(in_features=embedding_size, out_features=embedding_size, bias=False)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=embedding_size)\n",
        "    self.post_out = nn.Linear(in_features=embedding_size, out_features=500, bias=False)\n",
        "    self.bn4 = nn.BatchNorm1d(num_features=500)\n",
        "    \n",
        "\n",
        "    self.final = nn.Linear(in_features=500, out_features=1, bias = False)\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "\n",
        "    one_hot = one_hot.reshape(len(one_hot), 25*len(V))\n",
        "\n",
        "    x = F.relu(self.bn1(self.inputlayer(one_hot)))\n",
        "\n",
        "    x1 = self.nextlayer(x)\n",
        "\n",
        "    x1 = F.relu(self.bn2(x1))\n",
        "\n",
        "    x2 = torch.matmul(x1, self.fc1)\n",
        "\n",
        "    x3 = torch.matmul(x2, self.fc2)\n",
        "\n",
        "    x3 = F.relu(x3)\n",
        "\n",
        "    x4 = self.pre_out(x3)\n",
        "\n",
        "    x4 = F.relu(self.bn3(x4))\n",
        "\n",
        "    x5 = self.post_out(x4)\n",
        "\n",
        "    x5 = F.relu(self.bn4(x5))\n",
        "\n",
        "    x6 = (self.final(x5))\n",
        "\n",
        "    out = F.sigmoid(x6)\n",
        "    return out\n",
        "\n",
        "sentiment = Sentiment_Analysis(len(V), embedding_size)\n",
        "torch.cuda.empty_cache()\n",
        "if torch.cuda.is_available():\n",
        "    sentiment.cuda()\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Sx6wW3rlP2"
      },
      "source": [
        "# Define optimizer and loss\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = optim.Adam(sentiment.parameters(), lr=learning_rate)\n",
        "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "torch.cuda.empty_cache()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.BCELoss()\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "lsHSFjGjtTLb",
        "outputId": "a4eb3145-0ed0-4ebc-ec37-c8fa3ff17aba"
      },
      "source": [
        "# # Define train procedure\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "def train(dataset):\n",
        "  print(\"Training started\")\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  total_loss = 0\n",
        "  stopping_loss_threshold = 1.0\n",
        "  min_loss = 10000\n",
        "  early_stop = False\n",
        "  epochs_loss = []\n",
        "\n",
        "  for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    counter = 0\n",
        "    \n",
        "    for i, (inputs, labels) in enumerate(dataset):\n",
        "      counter += 1\n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "      sentiment.zero_grad()\n",
        "      log_probs = sentiment(inputs)\n",
        "      labels = labels.unsqueeze( 1)\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() \n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if i % 100 == 99:    # print every 100 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss/100))\n",
        "            epochs_loss.append(running_loss/100)\n",
        "\n",
        "            if (running_loss/100) < min_loss:\n",
        "              min_loss = running_loss/100\n",
        "            \n",
        "            running_loss = 0.0\n",
        "       \n",
        "  print('Minimum loss after 6 Epochs:', min_loss)\n",
        "  plt.plot(epochs_loss, label=\"Training Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "train(trainloader)\n",
        "\n",
        "print(\"Training finished\")\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training started\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0f468a00f31f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-0f468a00f31f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    group['eps'])\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.88 GiB (GPU 0; 14.76 GiB total capacity; 13.41 GiB already allocated; 307.75 MiB free; 13.42 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Hrw-3CtW6e"
      },
      "source": [
        "\n",
        "def test(dataset):\n",
        "    print(\"Testing started\")\n",
        "    count=0\n",
        "\n",
        "    hitcount=0\n",
        "    for inputs, labels in dataset: \n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "      \n",
        "      # 1. Generate predictions\n",
        "      optimizer.zero_grad()\n",
        "      # print(xb.shape)\n",
        "    \n",
        "      log_probs = sentiment(inputs)\n",
        "\n",
        " \n",
        "      labelscount=-1\n",
        "      for i in log_probs:\n",
        "        count+=1        \n",
        "        labelscount+=1\n",
        "        if i <0.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(0.0)):\n",
        "            hitcount=hitcount+1\n",
        "        elif i>=.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(1.0)):\n",
        "            hitcount=hitcount+1\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "      # 2. Calculate loss\n",
        "      \n",
        "      labels = labels.unsqueeze(1)\n",
        "\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('accuracy:', hitcount/count)\n",
        "\n",
        "\n",
        "test(testloader)\n",
        "\n",
        "print(\"Testing finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4AJ05l4_wx9"
      },
      "source": [
        "# Save the model\n",
        "PATH = './modelhindi.pt'\n",
        "torch.save(sentiment.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMr5_GvuAXkt"
      },
      "source": [
        "##task2**.2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gULs07PyCpD1"
      },
      "source": [
        "data = pd.read_csv(r'/content/bengali_hatespeech.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ1w2qJmAY3I"
      },
      "source": [
        "# Stopword Read\n",
        "stop_word_list = pd.read_csv(r'/content/banglastopword.txt', sep='\\s+', header=None)\n",
        "stop_word_list = stop_word_list[0].tolist() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkicvJMmBc1a"
      },
      "source": [
        "# Removing Punctuations and other unnecessary signs\n",
        "data['sentence'] = data['sentence'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "data['sentence'] = data['sentence'].str.replace('[{}]'.format('।'), '')\n",
        "data['sentence'] = data['sentence'].str.replace('[{}]'.format('\\n'), '')\n",
        "data['sentence'] = data['sentence'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_MK72HRBftu"
      },
      "source": [
        "#TODO: implement!\n",
        "# Removing stopwords\n",
        "data['sentence'] = data['sentence'].apply(lambda x: ' '.join([item for item in x.split() if item not in (stop_word_list)]))\n",
        "len(data['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEB05a2EChb6"
      },
      "source": [
        "data1 = data[6000:14280] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LG0jMAxUCvi"
      },
      "source": [
        "df = data[:7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5xxopYBDFIl"
      },
      "source": [
        "data1 = data1.append(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGL8_ISVDF22"
      },
      "source": [
        "data1 = data1.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zgRqT9EDwF-"
      },
      "source": [
        "data1 = data1.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcFQnnQrD2L8"
      },
      "source": [
        "if 'index' in data1:\n",
        "  del data1['index']\n",
        "\n",
        "if 'category' in data1:\n",
        "  del data1['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLe944nSRMOp"
      },
      "source": [
        "data = data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KOPUx4qECS7"
      },
      "source": [
        "V = list(data['sentence'].str.split(' ', expand=True).stack().unique())\n",
        "len(V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uXepMcdXfI4"
      },
      "source": [
        "# One hot grabbing function\n",
        "def word_to_one_hot(word, vocab):\n",
        "  return vocab[word]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeZKG3N0EJ5h"
      },
      "source": [
        "def all_word_corpus(data):\n",
        "  all_word_list = []\n",
        "  for value in data.str.split(\" \"):\n",
        "    all_word_list.extend(value)\n",
        "    # for each in value.split(\" \"):\n",
        "    #   # print(each)\n",
        "    # word_list.append(value)\n",
        "  return all_word_list\n",
        "\n",
        "all_word_list = all_word_corpus(data[\"sentence\"])\n",
        "len(all_word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nHGsOpsEO8p"
      },
      "source": [
        "#TODO: implement!\n",
        "\n",
        "# Subsampling function\n",
        "def sampling_prob(word, corpus):\n",
        "  word_number_list = []\n",
        "  word_freq_dict = {} # Each word frequency dictionary\n",
        "  word_prob_dict = {}\n",
        "  for each in word:\n",
        "    zw_i = corpus.count(each)/ len(corpus)\n",
        "    word_number_list.append(zw_i)\n",
        "    word_freq_dict[each] = zw_i\n",
        "    Pkeepw_i = (math.sqrt(zw_i/0.001) + 1) * (0.001/zw_i)\n",
        "    word_prob_dict[each] = Pkeepw_i\n",
        "  return word_freq_dict, word_prob_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4kQ8pY1EUwc"
      },
      "source": [
        "\n",
        "word_frequency_dictionary, word_probability_dictionary = sampling_prob(V, all_word_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ_mIWpsEXRg"
      },
      "source": [
        "# New unique word list(V), New Word frquency dictionary, new text data(data)\n",
        "def new_word_probability_dictionary(word_probability_dictionary, V, all_word_list, data):\n",
        "  threshold = 1.9\n",
        "  deleted_word_list = []\n",
        "  for i in word_probability_dictionary:\n",
        "    if word_probability_dictionary[i] <= threshold: \n",
        "      deleted_word_list.append(i)\n",
        "  data[\"sentence\"] = data[\"sentence\"].apply(lambda x: ' '.join([item for item in x.split() if item not in (deleted_word_list)]))\n",
        "  V = list(data[\"sentence\"].str.split(' ', expand=True).stack().unique())\n",
        "  all_word_list = all_word_corpus(data[\"sentence\"])\n",
        "  word_frequency_dictionary, word_probability_dictionary = sampling_prob(V, all_word_list)\n",
        "  return word_frequency_dictionary, word_probability_dictionary, V, data\n",
        "# print(\"word_probability_dictionary:\", len(word_probability_dictionary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8au9bvxIEhMs"
      },
      "source": [
        "word_frequency_dictionary, word_probability_dictionary, V, data= new_word_probability_dictionary(word_probability_dictionary, V, all_word_list, data)\n",
        "len(word_probability_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFH7Rwu3_NeL"
      },
      "source": [
        "df = pd.DataFrame(list(zip(V)), columns=['Vocalbulary'])\n",
        "one_hot_vocabulary = pd.get_dummies(df.Vocalbulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSzqk5eq_SVG"
      },
      "source": [
        "for each in range(len(data['sentence'])):\n",
        "  if len(data['sentence'][each].split(\" \"))<= 25:\n",
        "    for i in range(25 - len(data['sentence'][each].split(\" \"))):\n",
        "      data['sentence'][each] = data['sentence'][each] + \" 0\"\n",
        "  elif len(data['sentence'][each].split(\" \"))> 25:\n",
        "    x = data['sentence'][each].split(\" \")[:25]\n",
        "    data['sentence'][each] = \" \".join(x)\n",
        "  else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTpalesnEi_p"
      },
      "source": [
        "#TODO: implement!\n",
        "window_size = 4 # we define it later also.\n",
        "def get_target_context(sentence, V):\n",
        "  sentence_word_list = []\n",
        "  \n",
        "  for each in sentence.split(\" \"):\n",
        "    sentence_word_list.append(each)\n",
        "  \n",
        "  for i in sentence_word_list:\n",
        "    sentence_word_index = {}\n",
        "\n",
        "    # For sentence size more than 4\n",
        "    if len(sentence_word_list)>= window_size+1:\n",
        "      # \n",
        "      if (len(sentence_word_list) - sentence_word_list.index(i)) > 2:\n",
        "\n",
        "        if sentence_word_list.index(i)==0:\n",
        "          sentence_word_index[i] = [V.index(sentence_word_list[1]),V.index(sentence_word_list[2]),V.index(sentence_word_list[3]),V.index(sentence_word_list[4])]\n",
        "\n",
        "        elif sentence_word_list.index(i)==1:\n",
        "          sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[2]),V.index(sentence_word_list[3]),V.index(sentence_word_list[4])]\n",
        "        else:\n",
        "          index_num = sentence_word_list.index(i)\n",
        "          sentence_word_index[i] = [V.index(sentence_word_list[index_num-2]),V.index(sentence_word_list[index_num-1]),V.index(sentence_word_list[index_num+1]),V.index(sentence_word_list[index_num+2])]\n",
        "      elif (len(sentence_word_list) - sentence_word_list.index(i)) == 2:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[len(sentence_word_list)-5]),V.index(sentence_word_list[len(sentence_word_list)-4]),V.index(sentence_word_list[len(sentence_word_list)-3]),V.index(sentence_word_list[len(sentence_word_list)-1])]\n",
        "      else:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[len(sentence_word_list)-5]),V.index(sentence_word_list[len(sentence_word_list)-4]),V.index(sentence_word_list[len(sentence_word_list)-3]),V.index(sentence_word_list[len(sentence_word_list)-2])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    # For sentence size 4\n",
        "    elif len(sentence_word_list) == window_size:\n",
        "      if sentence_word_list.index(i) == 0:\n",
        "      \n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[1]),V.index(sentence_word_list[2]),V.index(sentence_word_list[3]),V.index(sentence_word_list[1])]\n",
        "     \n",
        "      elif sentence_word_list.index(i)==1:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[2]),V.index(sentence_word_list[3]),V.index(sentence_word_list[0])]\n",
        "      elif sentence_word_list.index(i)==2: \n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[1]),V.index(sentence_word_list[3]),V.index(sentence_word_list[0])]\n",
        "      else:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[1]),V.index(sentence_word_list[2]),V.index(sentence_word_list[0])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    # For sentence size 3\n",
        "    elif len(sentence_word_list) == window_size-1:\n",
        "      if sentence_word_list.index(i) == 0:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[1]),V.index(sentence_word_list[2]),V.index(sentence_word_list[1]),V.index(sentence_word_list[2])]\n",
        "      elif sentence_word_list.index(i)==1:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[2]),V.index(sentence_word_list[0]),V.index(sentence_word_list[2])]\n",
        "      else:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[1]),V.index(sentence_word_list[0]),V.index(sentence_word_list[1])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    # For sentence size 2\n",
        "    elif len(sentence_word_list) == window_size-2:\n",
        "      if sentence_word_list.index(i) == 0:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[1]),V.index(sentence_word_list[1]),V.index(sentence_word_list[1]),V.index(sentence_word_list[1])]\n",
        "      else:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[0]),V.index(sentence_word_list[0]),V.index(sentence_word_list[0])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    # For sentence size 1\n",
        "    elif len(sentence_word_list) == window_size-3:\n",
        "      sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[0]),V.index(sentence_word_list[0]),V.index(sentence_word_list[0])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    else:\n",
        "      pass\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x30U_VpDEvPR"
      },
      "source": [
        "# All (current_word, context) dictionary\n",
        "all_yield_wrods_context = {}\n",
        "for each in data:\n",
        "  gt = get_target_context(each, V)\n",
        "  for x in gt:\n",
        "    all_yield_wrods_context.update(x)\n",
        "\n",
        "len(all_yield_wrods_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaH6nj52fkh9"
      },
      "source": [
        "##2.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB0STysWExh1"
      },
      "source": [
        "# Set hyperparameters\n",
        "window_size = 4\n",
        "embedding_size = 300\n",
        "\n",
        "# More hyperparameters\n",
        "learning_rate = 0.05\n",
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJ8DTbfE4wj"
      },
      "source": [
        "# DataLoader\n",
        "def dataloader_for_model(all_yield_wrods_context, V):\n",
        "  word_to_one_hot_list_for_tensor = []\n",
        "  context_list_for_tensor = []\n",
        "  df = pd.DataFrame(list(zip(V)), columns=['Vocalbulary'])\n",
        "  one_hot_vocabulary = pd.get_dummies(df.Vocalbulary)\n",
        "  for i in all_yield_wrods_context:\n",
        "    word_to_one_hot_list_for_tensor.append(word_to_one_hot(i,one_hot_vocabulary))\n",
        "    context_list_for_tensor.append(all_yield_wrods_context[i])\n",
        "\n",
        "  one_hot_tensor = torch.FloatTensor(word_to_one_hot_list_for_tensor)\n",
        "  context_tensor = torch.FloatTensor(context_list_for_tensor)\n",
        "  # print(torch.max(context_tensor,1)[1])\n",
        "  dataset = TensorDataset(one_hot_tensor,context_tensor)\n",
        "\n",
        "\n",
        "  loader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=32,\n",
        "      num_workers=0,\n",
        "      shuffle=True\n",
        "  )\n",
        "  return loader\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8CEvsn2E7Zq"
      },
      "source": [
        "# Create model \n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, length, embedding_size):\n",
        "    super().__init__()\n",
        "    # print(\"First.\")\n",
        "    self.input_layer = nn.Linear(in_features=length, out_features=embedding_size, bias=False)\n",
        "    # print(\"Second.\")\n",
        "    self.output = nn.Linear(in_features=embedding_size, out_features=length, bias=False)\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "    x=self.input_layer(one_hot)\n",
        "    # print(\" x shape \",x.size())\n",
        "    x2 = self.output(x)\n",
        "    # print(\" x2 shape \",x2.size())\n",
        "    # out = nn.LogSigmoid(x2)\n",
        "    out = F.log_softmax(x2)\n",
        "    return out\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model = Word2Vec(len(word_probability_dictionary), embedding_size)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pjUF40cE6lU"
      },
      "source": [
        "# Previous model load.\n",
        "model = Word2Vec(20150, 300)\n",
        "model.load_state_dict(torch.load(\"model\"))\n",
        "# model.load_state_dict(torch.load(\"/content/gdrive/MyDrive/Colab Notebooks/model\", map_location=torch.device('cpu')))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb1VWNKGVyvU"
      },
      "source": [
        "# Define optimizer and loss\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.NLLLoss()\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTw31reyV8c-"
      },
      "source": [
        "# Define train procedure\n",
        "losses = []\n",
        "\n",
        "# load initial weights\n",
        "\n",
        "def train(dataset):\n",
        "  print(\"Training started\")\n",
        "  \n",
        "  total_loss = 0\n",
        "  stopping_loss_threshold = 1.0\n",
        "  early_stop = False\n",
        "  for epoch in range(epochs):\n",
        "    for i, data in enumerate(dataset):\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device, dtype=torch.int64)\n",
        "\n",
        "      #  current_word, context = current_word.cuda(), context.cuda()\n",
        "      #  current_word, context = current_word.to(device), context.to(device)\n",
        "      model.zero_grad()\n",
        "      log_probs = model(inputs)\n",
        "      loss = criterion(log_probs, torch.max(labels, 1)[1])\n",
        "      # loss = criterion(log_probs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      # if (total_loss / ) <=stopping_loss_threshold:\n",
        "      #   print(\"Early Stop.\")\n",
        "      #   print('epoch:%d, loss: %.3f' %\n",
        "      #       (epoch + 1, total_loss / 100))\n",
        "      #   early_stop = True\n",
        "      #   break\n",
        "      # else:\n",
        "      if i % 100 == 99:    # print every 100 mini-batches\n",
        "        print('epoch:%d, batch:%d, loss: %.3f' %\n",
        "          (epoch + 1, i + 1, total_loss / 100))\n",
        "        if (total_loss /100 ) <=stopping_loss_threshold:\n",
        "          print(\"Early Stop.\")\n",
        "          print('epoch:%d, loss: %.3f' %\n",
        "              (epoch + 1, total_loss / 100))\n",
        "          early_stop = True\n",
        "          break\n",
        "        total_loss = 0.0\n",
        "\n",
        "        # losses.append(total_loss/100)total_loss = 0.0\n",
        "      # total_loss = 0.0\n",
        "\n",
        "    # print(losses)\n",
        "\n",
        "    if early_stop:\n",
        "      print(\"Stopped\")\n",
        "      break\n",
        "   \n",
        "train(dataloader_for_model(all_yield_wrods_context, V))\n",
        "\n",
        "print(\"Training finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPDOH07gV_4v"
      },
      "source": [
        "# Save the model\n",
        "PATH = './modelbengali.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIJZvBaZfdX3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0REWRuBfyp3"
      },
      "source": [
        "##2.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqGiE76_f0ee"
      },
      "source": [
        "class sample_data():\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,data):\n",
        "      self.data = data\n",
        "    def __len__(self):\n",
        "       return len(self.data)        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      word_to_one_hot_list_for_tensor = []\n",
        "      for each in data[\"sentence\"][idx].split(' '):\n",
        "        # word_to_one_hot_list_for_tensor.append(word_to_one_hot(each,one_hot_vocabulary))\n",
        "        for i in V:\n",
        "          if i in each and each in i:\n",
        "            word_to_one_hot_list_for_tensor.append(1)\n",
        "          else:\n",
        "            word_to_one_hot_list_for_tensor.append(0)\n",
        "\n",
        "\n",
        "      _label=0.0\n",
        "      if self.data['hate'][idx]==1:\n",
        "        _label=1.0\n",
        "      return torch.Tensor(word_to_one_hot_list_for_tensor), torch.tensor(_label)\n",
        "\n",
        "\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnCsTRe1iGu8"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL-3u2RbiHto"
      },
      "source": [
        "sample_custom_dataset = sample_data(data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nVBK9xeiZ29"
      },
      "source": [
        "tr_size = int(0.8 * len(sample_custom_dataset))\n",
        "te_size = len(sample_custom_dataset) - tr_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(sample_custom_dataset, [tr_size, te_size])\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size= 32, shuffle = True)\n",
        "testloader = DataLoader(test_dataset, batch_size= 32, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iacY4yN3gXf"
      },
      "source": [
        "sentiment = Sentiment_Analysis(len(V), embedding_size)\n",
        "torch.cuda.empty_cache()\n",
        "if torch.cuda.is_available():\n",
        "    sentiment.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqCC4KomBl-L"
      },
      "source": [
        "sentiment.load_state_dict(torch.load(\"/content/modelhindi.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auIKuvsn827G"
      },
      "source": [
        "# Define optimizer and loss\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = optim.Adam(sentiment.parameters(), lr=learning_rate)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phDWJndU88IR"
      },
      "source": [
        "\n",
        "def test(dataset):\n",
        "    print(\"Testing started\")\n",
        "    count=0\n",
        "\n",
        "    hitcount=0\n",
        "    for inputs, labels in dataset: \n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "      \n",
        "      # 1. Generate predictions\n",
        "      optimizer.zero_grad()\n",
        "      # print(xb.shape)\n",
        "    \n",
        "      log_probs = sentiment(inputs)\n",
        "\n",
        " \n",
        "      labelscount=-1\n",
        "      for i in log_probs:\n",
        "        count+=1        \n",
        "        labelscount+=1\n",
        "        if i <0.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(0.0)):\n",
        "            hitcount=hitcount+1\n",
        "        elif i>=.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(1.0)):\n",
        "            hitcount=hitcount+1\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "      # 2. Calculate loss\n",
        "      \n",
        "      labels = labels.unsqueeze(1)\n",
        "\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('accuracy:', hitcount/count)\n",
        "\n",
        "\n",
        "test(testloader)\n",
        "\n",
        "print(\"Testing finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZeExvmf8_L5"
      },
      "source": [
        "# # Define train procedure\n",
        "# losses = []\n",
        "\n",
        "# # load initial weights\n",
        "# torch.cuda.empty_cache()\n",
        "def train(dataset):\n",
        "  print(\"Training started\")\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  total_loss = 0\n",
        "  stopping_loss_threshold = 1.0\n",
        "  min_loss = 10000\n",
        "  early_stop = False\n",
        "  epochs_loss = []\n",
        "\n",
        "  for epoch in range(10):\n",
        "    print(len(dataset))\n",
        "    running_loss = 0.0\n",
        "    counter = 0\n",
        "    \n",
        "    for i, (inputs, labels) in enumerate(dataset):\n",
        "      counter += 1\n",
        "\n",
        "      # inputs,labels=inputs.type(torch.int),labels.type(torch.int)\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      # labels = labels.unsqueeze(1)\n",
        "      # print(labels.shape)\n",
        "      # print(inputs.shape)\n",
        "\n",
        "\n",
        "\n",
        "      sentiment.zero_grad()\n",
        "      # inputs = torch.tensor(inputs).to(torch.long)\n",
        "      log_probs = sentiment(inputs)\n",
        "      # print(len(log_probs[0][0]))\n",
        "      # loss = criterion(log_probs, torch.max(labels, 1)[1])\n",
        "      labels = labels.unsqueeze( 1)\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "      # loss = criterion(log_probs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() \n",
        "      total_loss += loss.item()\n",
        "\n",
        "    print('epoch: %d, loss: %.3f' %\n",
        "                  (epoch + 1, running_loss/208))\n",
        "    epochs_loss.append(running_loss/208)\n",
        "    if (running_loss/208) < min_loss:\n",
        "      min_loss = running_loss/207\n",
        "            \n",
        "  print('Minimum loss after 10 Epochs:', min_loss)\n",
        "  plt.plot(epochs_loss, label=\"Training Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "   \n",
        "train(trainloader)\n",
        "\n",
        "print(\"Training finished\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEhFKl6xCdJV"
      },
      "source": [
        "\n",
        "def test(dataset):\n",
        "    print(\"Testing started\")\n",
        "    count=0\n",
        "\n",
        "    hitcount=0\n",
        "    for inputs, labels in dataset: \n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "      \n",
        "      # 1. Generate predictions\n",
        "      optimizer.zero_grad()\n",
        "      # print(xb.shape)\n",
        "    \n",
        "      log_probs = sentiment(inputs)\n",
        "\n",
        " \n",
        "      labelscount=-1\n",
        "      for i in log_probs:\n",
        "        count+=1        \n",
        "        labelscount+=1\n",
        "        if i <0.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(0.0)):\n",
        "            hitcount=hitcount+1\n",
        "        elif i>=.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(1.0)):\n",
        "            hitcount=hitcount+1\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "      # 2. Calculate loss\n",
        "      \n",
        "      labels = labels.unsqueeze(1)\n",
        "\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('accuracy:', hitcount/count)\n",
        "\n",
        "\n",
        "test(testloader)\n",
        "\n",
        "print(\"Testing finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9AdLnXIVcsc"
      },
      "source": [
        "# Save the model\n",
        "PATH = './combine.pt'\n",
        "torch.save(sentiment.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}