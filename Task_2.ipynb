{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of task2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au7iOZPtk5pb"
      },
      "source": [
        "#**NN Project**\n",
        "### Prajvi Saxena\n",
        "### Md. Jonybul Islam\n",
        "\n",
        "Code for Task 2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZIENPTPl5QM"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import string \n",
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtX1c4ivl7MY"
      },
      "source": [
        "# upload files \n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqAlFhe5m89d"
      },
      "source": [
        "# stopword and punctuations removal\n",
        "data = pd.read_csv('hindi_hatespeech.tsv', sep='\\t')\n",
        "stop_word_list = pd.read_csv('stopword.txt', sep='\\s+', header=None)\n",
        "stop_word_list = stop_word_list[0].tolist()\n",
        "data['text'] = data['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "data['text'] = data['text'].str.replace('[{}]'.format('।'), '')\n",
        "data['text'] = data['text'].str.lower()\n",
        "data['text'] = data['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in (stop_word_list)]))\n",
        "data['task_1'] = data['task_1'].map({'HOF': 1, 'NOT': 0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kml-YtPPmGEi"
      },
      "source": [
        "# getting list of wunique words\n",
        "V = list(data['text'].str.split(' ', expand=True).stack().unique())\n",
        "print(len(V))\n",
        "def word_to_one_hot(word, vocab):\n",
        "  return vocab[word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDfIX195mk6q"
      },
      "source": [
        "# getting all words in the corpus\n",
        "def all_word_corpus(data):\n",
        "  all_word_list = []\n",
        "  for value in data.str.split(\" \"):\n",
        "    all_word_list.extend(value)\n",
        "    # for each in value.split(\" \"):\n",
        "    #   # print(each)\n",
        "    # word_list.append(value)\n",
        "  return all_word_list\n",
        "\n",
        "all_word_list = all_word_corpus(data[\"text\"])\n",
        "len(all_word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h22_HiktnmHQ"
      },
      "source": [
        "\n",
        "# function for Subsampling\n",
        "def sampling_prob(word, corpus):\n",
        "  word_number_list = []\n",
        "  word_freq_dict = {} # Each word frequency dictionary\n",
        "  word_prob_dict = {}\n",
        "  for each in word:\n",
        "    zw_i = corpus.count(each)/ len(corpus)\n",
        "    word_number_list.append(zw_i)\n",
        "    word_freq_dict[each] = zw_i\n",
        "    Pkeepw_i = (math.sqrt(zw_i/0.001) + 1) * (0.001/zw_i)\n",
        "    word_prob_dict[each] = Pkeepw_i\n",
        "  return word_freq_dict, word_prob_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pQnHxJjnr-n"
      },
      "source": [
        "word_frequency_dictionary, word_probability_dictionary = sampling_prob(V, all_word_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xTm2lg8nvUo"
      },
      "source": [
        "# New unique word list(V), New Word frquency dictionary, new text data(data)\n",
        "def new_word_probability_dictionary(word_probability_dictionary, V, all_word_list, data):\n",
        "  threshold = 1.9\n",
        "  deleted_word_list = []\n",
        "  for i in word_probability_dictionary:\n",
        "    if word_probability_dictionary[i] <= threshold: \n",
        "      deleted_word_list.append(i)\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda x: ' '.join([item for item in x.split() if item not in (deleted_word_list)]))\n",
        "  V = list(data[\"text\"].str.split(' ', expand=True).stack().unique())\n",
        "  all_word_list = all_word_corpus(data[\"text\"])\n",
        "  word_frequency_dictionary, word_probability_dictionary = sampling_prob(V, all_word_list)\n",
        "  return word_frequency_dictionary, word_probability_dictionary, V, data\n",
        "# print(\"word_probability_dictionary:\", len(word_probability_dictionary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSsvTPJfn-wN"
      },
      "source": [
        "word_frequency_dictionary, word_probability_dictionary, V, data= new_word_probability_dictionary(word_probability_dictionary, V, all_word_list, data)\n",
        "len(word_probability_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO1GPuZUoA_E"
      },
      "source": [
        "# one hot vocabulary\n",
        "df = pd.DataFrame(list(zip(V)), columns=['Vocalbulary'])\n",
        "one_hot_vocabulary = pd.get_dummies(df.Vocalbulary)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTyjUr2coKHn"
      },
      "source": [
        "# splitting each sentence of size 25, adding padding of 0's if less than 25.\n",
        "# to make of same size\n",
        "for each in range(len(data['text'])):\n",
        "  if len(data['text'][each].split(\" \"))<= 25:\n",
        "    for i in range(25 - len(data['text'][each].split(\" \"))):\n",
        "      data['text'][each] = data['text'][each] + \" 0\"\n",
        "  elif len(data['text'][each].split(\" \"))> 25:\n",
        "    x = data['text'][each].split(\" \")[:25]\n",
        "    data['text'][each] = \" \".join(x)\n",
        "  else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU7Dhx9loNOq"
      },
      "source": [
        "# model structure for word2vec embedding model\n",
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, length, embedding_size):\n",
        "    super().__init__()\n",
        "    self.input_layer = nn.Linear(in_features=length, out_features=embedding_size, bias=False)\n",
        "    self.output = nn.Linear(in_features=embedding_size, out_features=length, bias=False)\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "    x=self.input_layer(one_hot)\n",
        "    x2 = self.output(x)\n",
        "    out = F.log_softmax(x2)\n",
        "    return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yD9WoXmoSja"
      },
      "source": [
        "# Previous model load.\n",
        "model = Word2Vec(20150, 300)\n",
        "model.load_state_dict(torch.load(\"model\"))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t81vVez6rGE_"
      },
      "source": [
        "# Set hyperparameters\n",
        "window_size = 4\n",
        "embedding_size = 300\n",
        "learning_rate = 0.001\n",
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8icroA_rLXY"
      },
      "source": [
        "# getting data\n",
        "class sample_data():\n",
        "\n",
        "    def __init__(self,data):\n",
        "      self.data = data\n",
        "    def __len__(self):\n",
        "       return len(self.data)        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      word_to_one_hot_list_for_tensor = []\n",
        "      for each in data[\"text\"][idx].split(' '):\n",
        "        word_to_one_hot_list_for_tensor.append(word_to_one_hot(each,one_hot_vocabulary))\n",
        "\n",
        "      _label=0.0\n",
        "      if self.data['task_1'][idx]==1:\n",
        "        _label=1.0\n",
        "      return torch.Tensor(word_to_one_hot_list_for_tensor), torch.tensor(_label)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uB9LqUprQ3t"
      },
      "source": [
        "sample_custom_dataset = sample_data(data)\n",
        "print(len(sample_custom_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHGWCLAMsaYf"
      },
      "source": [
        "# dividing data in 80% training and 20% test\n",
        "# loading data with batch size 12. \n",
        "\n",
        "tr_size = int(0.8 * len(sample_custom_dataset))\n",
        "# print(tr_size)\n",
        "te_size = len(sample_custom_dataset) - tr_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(sample_custom_dataset, [tr_size, te_size])\n",
        "# print(len(train_dataset))\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size= 12, shuffle = True)\n",
        "# print(len(trainloader))\n",
        "testloader = DataLoader(test_dataset, batch_size= 12, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rvBRJfHraoC"
      },
      "source": [
        "# Classifier model definition and forward pass\n",
        "\n",
        "class Sentiment_Analysis(nn.Module):\n",
        "  def __init__(self, length, embedding_size):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.layer1 = nn.Linear(in_features=20150*25, out_features=1000, bias=False) \n",
        "    self.bn1 = nn.BatchNorm1d(num_features=1000)\n",
        "\n",
        "    self.layer2 = nn.Linear(in_features=1000, out_features=300, bias=False)\n",
        "    self.bn2 = nn.BatchNorm1d(num_features=300) \n",
        "\n",
        "    self.fc1 = model.input_layer.weight\n",
        "    self.fc2 = model.output.weight\n",
        "\n",
        "    self.layer5 = nn.Linear(in_features=embedding_size, out_features=embedding_size, bias=False)\n",
        "    self.bn3 = nn.BatchNorm1d(num_features=embedding_size)\n",
        "    self.layer6 = nn.Linear(in_features=embedding_size, out_features=500, bias=False)\n",
        "    self.bn4 = nn.BatchNorm1d(num_features=500)\n",
        "    \n",
        "    self.final = nn.Linear(in_features=500, out_features=1, bias = False)\n",
        "\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "\n",
        "    one_hot = one_hot.reshape(len(one_hot), 25*len(V))\n",
        "\n",
        "    x = torch.relu(self.bn1(self.layer1(one_hot)))\n",
        "    x = torch.relu(self.bn2(self.layer2(x)))\n",
        "\n",
        "    x1 = torch.matmul(x, self.fc1)\n",
        "    x2 = torch.matmul(x1, self.fc2)\n",
        "    x2 = torch.relu(x2)\n",
        "\n",
        "    x = torch.relu(self.bn3(self.layer5(x2)))\n",
        "\n",
        "    x = torch.relu(self.bn4(self.layer6(x)))\n",
        "\n",
        "    x = (self.final(x))\n",
        "\n",
        "    out = torch.sigmoid(x)\n",
        "    return out\n",
        "\n",
        "sentiment = Sentiment_Analysis(len(V), embedding_size)\n",
        "torch.cuda.empty_cache()\n",
        "if torch.cuda.is_available():\n",
        "    sentiment.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1Sx6wW3rlP2"
      },
      "source": [
        "# Define Adam optimizer and BCE loss\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = optim.Adam(sentiment.parameters(), lr=learning_rate)\n",
        "torch.cuda.empty_cache()\n",
        "criterion = nn.BCELoss()\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsHSFjGjtTLb"
      },
      "source": [
        "# Train sentiment classifier model for hindi\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "def train(dataset):\n",
        "  print(\"Training started\")\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  total_loss = 0\n",
        "  stopping_loss_threshold = 1.0\n",
        "  min_loss = 10000\n",
        "  early_stop = False\n",
        "  epochs_loss = []\n",
        "\n",
        "  # loop for epochs\n",
        "  # stopping criteria is using early stopping\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for i, (inputs, labels) in enumerate(dataset):\n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      sentiment.zero_grad()\n",
        "      # getting predcited output\n",
        "      log_probs = sentiment(inputs)\n",
        "      labels = labels.unsqueeze( 1)\n",
        "      # getting loss\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() \n",
        "\n",
        "    print('epoch: %d, loss: %.3f' %\n",
        "                  (epoch + 1, running_loss/len(dataset)))\n",
        "    epochs_loss.append(running_loss/len(dataset))\n",
        "    # Early stopping\n",
        "    if (running_loss/len(dataset)) < min_loss:\n",
        "      min_loss = running_loss/len(dataset)\n",
        "      model_state_to_save = sentiment.state_dict()\n",
        "    else:\n",
        "      print(\"Early Stopped\")\n",
        "      break\n",
        "\n",
        "    \n",
        "       \n",
        "  print('Minimum loss after early stop:', min_loss)\n",
        "  plt.plot(epochs_loss, label=\"Training Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  PATH = './modelhindi.pt'\n",
        "  torch.save(model_state_to_save, PATH)\n",
        "\n",
        "train(trainloader)\n",
        "\n",
        "print(\"Training finished\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0g2PYYQ01Y4"
      },
      "source": [
        "# sentiment = Sentiment_Analysis(len(V), embedding_size)\n",
        "# torch.cuda.empty_cache()\n",
        "# if torch.cuda.is_available():\n",
        "#     sentiment.cuda()\n",
        "\n",
        "# sentiment.load_state_dict(torch.load(\"/content/modelhindi.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Hrw-3CtW6e"
      },
      "source": [
        "# Testing on hindi dataset\n",
        "def test(dataset):\n",
        "    print(\"Testing started\")\n",
        "    count=0\n",
        "    \n",
        "    hitcount=0\n",
        "    for inputs, labels in dataset: \n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()    \n",
        "      log_probs = sentiment(inputs)\n",
        "\n",
        "      labelscount=-1\n",
        "      # getting the count of correct classification(hitcount)\n",
        "      for i in log_probs:\n",
        "        count+=1        \n",
        "        labelscount+=1\n",
        "        if i <0.5: \n",
        "          if torch.eq(labels[labelscount], torch.tensor(0.0)):\n",
        "            hitcount=hitcount+1\n",
        "        elif i>=.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(1.0)):\n",
        "            hitcount=hitcount+1\n",
        "      \n",
        "      labels = labels.unsqueeze(1)\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "    print('accuracy:', hitcount/count)\n",
        "\n",
        "test(testloader)\n",
        "\n",
        "print(\"Testing finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMr5_GvuAXkt"
      },
      "source": [
        "##task2**.2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gULs07PyCpD1"
      },
      "source": [
        "# loading bengali dataset\n",
        "data = pd.read_csv(r'/content/bengali_hatespeech.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ1w2qJmAY3I"
      },
      "source": [
        "# Stopword list reader\n",
        "stop_word_list = pd.read_csv(r'/content/banglastopword.txt', sep='\\s+', header=None)\n",
        "stop_word_list = stop_word_list[0].tolist() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkicvJMmBc1a"
      },
      "source": [
        "# Removing Punctuations and other unnecessary signs\n",
        "data['sentence'] = data['sentence'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "data['sentence'] = data['sentence'].str.replace('[{}]'.format('।'), '')\n",
        "data['sentence'] = data['sentence'].str.replace('[{}]'.format('\\n'), '')\n",
        "data['sentence'] = data['sentence'].str.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_MK72HRBftu"
      },
      "source": [
        "# Removing stopwords\n",
        "data['sentence'] = data['sentence'].apply(lambda x: ' '.join([item for item in x.split() if item not in (stop_word_list)]))\n",
        "len(data['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEB05a2EChb6"
      },
      "source": [
        "# slicing the dataset to make it of same size as hindi dataset\n",
        "data1 = data[6000:14280] \n",
        "df = data[:7]\n",
        "data1 = data1.append(df)\n",
        "data1 = data1.sample(frac=1)\n",
        "data1 = data1.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcFQnnQrD2L8"
      },
      "source": [
        "if 'index' in data1:\n",
        "  del data1['index']\n",
        "\n",
        "if 'category' in data1:\n",
        "  del data1['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSzqk5eq_SVG"
      },
      "source": [
        "# splitting the sentences with 25 words, if less than 25, we add a padding \n",
        "for each in range(len(data['sentence'])):\n",
        "  if len(data['sentence'][each].split(\" \"))<= 25:\n",
        "    for i in range(25 - len(data['sentence'][each].split(\" \"))):\n",
        "      data['sentence'][each] = data['sentence'][each] + \" জয়\"\n",
        "  elif len(data['sentence'][each].split(\" \"))> 25:\n",
        "    x = data['sentence'][each].split(\" \")[:25]\n",
        "    data['sentence'][each] = \" \".join(x)\n",
        "  else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLe944nSRMOp"
      },
      "source": [
        "data = data1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KOPUx4qECS7"
      },
      "source": [
        "# getting list of unique words\n",
        "V = list(data['sentence'].str.split(' ', expand=True).stack().unique())\n",
        "# print(V.index('0'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uXepMcdXfI4"
      },
      "source": [
        "# One hot grabbing function\n",
        "def word_to_one_hot(word, vocab):\n",
        "  return vocab[word]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeZKG3N0EJ5h"
      },
      "source": [
        "# all words in the corpus\n",
        "def all_word_corpus(data):\n",
        "  all_word_list = []\n",
        "  for value in data.str.split(\" \"):\n",
        "    all_word_list.extend(value)\n",
        "\n",
        "  return all_word_list\n",
        "\n",
        "all_word_list = all_word_corpus(data[\"sentence\"])\n",
        "len(all_word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nHGsOpsEO8p"
      },
      "source": [
        "# Subsampling function\n",
        "def sampling_prob(word, corpus):\n",
        "  word_number_list = []\n",
        "  word_freq_dict = {} # Each word frequency dictionary\n",
        "  word_prob_dict = {}\n",
        "  for each in word:\n",
        "    zw_i = corpus.count(each)/ len(corpus)\n",
        "    \n",
        "    word_number_list.append(zw_i)\n",
        "    word_freq_dict[each] = zw_i\n",
        "    Pkeepw_i = (math.sqrt(zw_i/0.001) + 1) * (0.001/zw_i)\n",
        "    word_prob_dict[each] = Pkeepw_i\n",
        "  return word_freq_dict, word_prob_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4kQ8pY1EUwc"
      },
      "source": [
        "word_frequency_dictionary, word_probability_dictionary = sampling_prob(V, all_word_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ_mIWpsEXRg"
      },
      "source": [
        "# getting word probability dictionary\n",
        "def new_word_probability_dictionary(word_probability_dictionary, V, all_word_list, data):\n",
        "  threshold = 1.9\n",
        "  deleted_word_list = []\n",
        "\n",
        "  for i in word_probability_dictionary:\n",
        "    if word_probability_dictionary[i] <= threshold: \n",
        "      deleted_word_list.append(i)\n",
        "  data[\"sentence\"] = data[\"sentence\"].apply(lambda x: ' '.join([item for item in x.split() if item not in (deleted_word_list)]))\n",
        "  V = list(data[\"sentence\"].str.split(' ', expand=True).stack().unique())\n",
        "  all_word_list = all_word_corpus(data[\"sentence\"])\n",
        "  word_frequency_dictionary, word_probability_dictionary = sampling_prob(V, all_word_list)\n",
        "  return word_frequency_dictionary, word_probability_dictionary, V, data\n",
        "# print(\"word_probability_dictionary:\", len(word_probability_dictionary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8au9bvxIEhMs"
      },
      "source": [
        "word_frequency_dictionary, word_probability_dictionary, V, data= new_word_probability_dictionary(word_probability_dictionary, V, all_word_list, data)\n",
        "len(word_probability_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFH7Rwu3_NeL"
      },
      "source": [
        "df = pd.DataFrame(list(zip(V)), columns=['Vocalbulary'])\n",
        "one_hot_vocabulary = pd.get_dummies(df.Vocalbulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTpalesnEi_p"
      },
      "source": [
        "# skipgram\n",
        "window_size = 4 \n",
        "def get_target_context(sentence, V):\n",
        "  sentence_word_list = []\n",
        "  \n",
        "  for each in sentence.split(\" \"):\n",
        "    sentence_word_list.append(each)\n",
        "  \n",
        "  for i in sentence_word_list:\n",
        "    sentence_word_index = {}\n",
        "\n",
        "    # For sentence size more than 4\n",
        "    if len(sentence_word_list)>= window_size+1:\n",
        "      # \n",
        "      if (len(sentence_word_list) - sentence_word_list.index(i)) > 2:\n",
        "\n",
        "        if sentence_word_list.index(i)==0:\n",
        "          sentence_word_index[i] = [V.index(sentence_word_list[1]),V.index(sentence_word_list[2]),V.index(sentence_word_list[3]),V.index(sentence_word_list[4])]\n",
        "\n",
        "        elif sentence_word_list.index(i)==1:\n",
        "          sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[2]),V.index(sentence_word_list[3]),V.index(sentence_word_list[4])]\n",
        "        else:\n",
        "          index_num = sentence_word_list.index(i)\n",
        "          sentence_word_index[i] = [V.index(sentence_word_list[index_num-2]),V.index(sentence_word_list[index_num-1]),V.index(sentence_word_list[index_num+1]),V.index(sentence_word_list[index_num+2])]\n",
        "      elif (len(sentence_word_list) - sentence_word_list.index(i)) == 2:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[len(sentence_word_list)-5]),V.index(sentence_word_list[len(sentence_word_list)-4]),V.index(sentence_word_list[len(sentence_word_list)-3]),V.index(sentence_word_list[len(sentence_word_list)-1])]\n",
        "      else:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[len(sentence_word_list)-5]),V.index(sentence_word_list[len(sentence_word_list)-4]),V.index(sentence_word_list[len(sentence_word_list)-3]),V.index(sentence_word_list[len(sentence_word_list)-2])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    # For sentence size 4\n",
        "    elif len(sentence_word_list) == window_size:\n",
        "      if sentence_word_list.index(i) == 0:\n",
        "      \n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[1]),V.index(sentence_word_list[2]),V.index(sentence_word_list[3]),V.index(sentence_word_list[1])]\n",
        "     \n",
        "      elif sentence_word_list.index(i)==1:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[2]),V.index(sentence_word_list[3]),V.index(sentence_word_list[0])]\n",
        "      elif sentence_word_list.index(i)==2: \n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[1]),V.index(sentence_word_list[3]),V.index(sentence_word_list[0])]\n",
        "      else:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[1]),V.index(sentence_word_list[2]),V.index(sentence_word_list[0])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    # For sentence size 3\n",
        "    elif len(sentence_word_list) == window_size-1:\n",
        "      if sentence_word_list.index(i) == 0:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[1]),V.index(sentence_word_list[2]),V.index(sentence_word_list[1]),V.index(sentence_word_list[2])]\n",
        "      elif sentence_word_list.index(i)==1:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[2]),V.index(sentence_word_list[0]),V.index(sentence_word_list[2])]\n",
        "      else:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[1]),V.index(sentence_word_list[0]),V.index(sentence_word_list[1])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    # For sentence size 2\n",
        "    elif len(sentence_word_list) == window_size-2:\n",
        "      if sentence_word_list.index(i) == 0:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[1]),V.index(sentence_word_list[1]),V.index(sentence_word_list[1]),V.index(sentence_word_list[1])]\n",
        "      else:\n",
        "        sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[0]),V.index(sentence_word_list[0]),V.index(sentence_word_list[0])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    # For sentence size 1\n",
        "    elif len(sentence_word_list) == window_size-3:\n",
        "      sentence_word_index[i] = [V.index(sentence_word_list[0]),V.index(sentence_word_list[0]),V.index(sentence_word_list[0]),V.index(sentence_word_list[0])]\n",
        "      yield sentence_word_index\n",
        "\n",
        "    else:\n",
        "      pass\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x30U_VpDEvPR"
      },
      "source": [
        "# All (current_word, context) dictionary\n",
        "all_yield_wrods_context = {}\n",
        "for each in data['sentence']:\n",
        "  # print(each)\n",
        "  gt = get_target_context(each, V)\n",
        "  for x in gt:\n",
        "    all_yield_wrods_context.update(x)\n",
        "\n",
        "len(all_yield_wrods_context)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaH6nj52fkh9"
      },
      "source": [
        "##2.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB0STysWExh1"
      },
      "source": [
        "# Set hyperparameters\n",
        "window_size = 4\n",
        "embedding_size = 300\n",
        "learning_rate = 0.05\n",
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJ8DTbfE4wj"
      },
      "source": [
        "# DataLoader for embeddings\n",
        "def dataloader_for_model(all_yield_wrods_context, V):\n",
        "  word_to_one_hot_list_for_tensor = []\n",
        "  context_list_for_tensor = []\n",
        "  df = pd.DataFrame(list(zip(V)), columns=['Vocalbulary'])\n",
        "  one_hot_vocabulary = pd.get_dummies(df.Vocalbulary)\n",
        "  for i in all_yield_wrods_context:\n",
        "    word_to_one_hot_list_for_tensor.append(word_to_one_hot(i,one_hot_vocabulary))\n",
        "    context_list_for_tensor.append(all_yield_wrods_context[i])\n",
        "\n",
        "  one_hot_tensor = torch.FloatTensor(word_to_one_hot_list_for_tensor)\n",
        "  context_tensor = torch.FloatTensor(context_list_for_tensor)\n",
        "  # print(torch.max(context_tensor,1)[1])\n",
        "  dataset = TensorDataset(one_hot_tensor,context_tensor)\n",
        "\n",
        "\n",
        "  loader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=32,\n",
        "      num_workers=0,\n",
        "      shuffle=True\n",
        "  )\n",
        "  return loader\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8CEvsn2E7Zq"
      },
      "source": [
        "# word2vec embedding model definition and forward function\n",
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, length, embedding_size):\n",
        "    super().__init__()\n",
        "    self.input_layer = nn.Linear(in_features=length, out_features=embedding_size, bias=False)\n",
        "    self.output = nn.Linear(in_features=embedding_size, out_features=length, bias=False)\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "    x=self.input_layer(one_hot)\n",
        "    x = self.output(x)\n",
        "    \n",
        "    out = F.log_softmax(x)\n",
        "    return out\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model = Word2Vec(len(word_probability_dictionary), embedding_size)\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pjUF40cE6lU"
      },
      "source": [
        "# Previous model load.\n",
        "model = Word2Vec(20150, 300)\n",
        "model.load_state_dict(torch.load(\"model\"))\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb1VWNKGVyvU"
      },
      "source": [
        "# Define Adam optimizer and NLL loss\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.NLLLoss()\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTw31reyV8c-"
      },
      "source": [
        "# Define train procedure for word2vec embeddings\n",
        "losses = []\n",
        "\n",
        "def train(dataset):\n",
        "  print(\"Training started\")\n",
        "  \n",
        "  total_loss = 0\n",
        "  min_loss = 10000\n",
        "  stopping_loss_threshold = 1.0\n",
        "  early_stop = False\n",
        "  epochs_loss = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(dataset):\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device, dtype=torch.int64)\n",
        "\n",
        "      model.zero_grad()\n",
        "      log_probs = model(inputs)\n",
        "      loss = criterion(log_probs, torch.max(labels, 1)[1])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      #early stopping\n",
        "      if i % 100 == 99:    # print every 100 mini-batches\n",
        "        print('epoch:%d, batch:%d, loss: %.3f' %\n",
        "          (epoch + 1, i + 1, total_loss / 100))\n",
        "        if (total_loss /100 ) <=stopping_loss_threshold:\n",
        "          print(\"Early Stop.\")\n",
        "          print('epoch:%d, loss: %.3f' %\n",
        "              (epoch + 1, total_loss / 100))\n",
        "          early_stop = True\n",
        "          break\n",
        "        total_loss = 0.0\n",
        "\n",
        "    if early_stop:\n",
        "      print(\"Stopped\")\n",
        "      break\n",
        "   \n",
        "train(dataloader_for_model(all_yield_wrods_context, V))\n",
        "\n",
        "print(\"Training finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPDOH07gV_4v"
      },
      "source": [
        "# Save the model\n",
        "PATH = './modelbengali.pt'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0REWRuBfyp3"
      },
      "source": [
        "##2.4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqGiE76_f0ee"
      },
      "source": [
        "#sample function for bangali dataset\n",
        "class sample_data():\n",
        "    def __init__(self,data):\n",
        "      self.data = data\n",
        "    def __len__(self):\n",
        "       return len(self.data)        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "      word_to_one_hot_list_for_tensor = []\n",
        "      for each in data[\"sentence\"][idx].split(' '):\n",
        "        for i in V:\n",
        "          if i in each and each in i:\n",
        "            word_to_one_hot_list_for_tensor.append(1)\n",
        "          else:\n",
        "            word_to_one_hot_list_for_tensor.append(0)\n",
        "\n",
        "      _label=0.0\n",
        "      if self.data['hate'][idx]==1:\n",
        "        _label=1.0\n",
        "      return torch.Tensor(word_to_one_hot_list_for_tensor), torch.tensor(_label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL-3u2RbiHto"
      },
      "source": [
        "sample_custom_dataset = sample_data(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nVBK9xeiZ29"
      },
      "source": [
        "# dividing dataset to 80% training and 20% test set\n",
        "# dataloader with batch size 32\n",
        "tr_size = int(0.8 * len(sample_custom_dataset))\n",
        "te_size = len(sample_custom_dataset) - tr_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(sample_custom_dataset, [tr_size, te_size])\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size= 32, shuffle = True)\n",
        "testloader = DataLoader(test_dataset, batch_size= 32, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqCC4KomBl-L"
      },
      "source": [
        "# loading the classifier model trained for hindi dataset in task 2.1\n",
        "sentiment.load_state_dict(torch.load(\"/content/modelhindi.pt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auIKuvsn827G"
      },
      "source": [
        "# Define Adam optimizer and BCE loss\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = optim.Adam(sentiment.parameters(), lr=learning_rate)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phDWJndU88IR"
      },
      "source": [
        "# testing bengali dataset on classifier trained on hindi dataset(2.1)\n",
        "def test(dataset):\n",
        "    print(\"Testing started\")\n",
        "    count=0\n",
        "\n",
        "    hitcount=0\n",
        "    for inputs, labels in dataset: \n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "      # 1. Generate predictions\n",
        "      optimizer.zero_grad()\n",
        "      log_probs = sentiment(inputs)\n",
        "\n",
        "      # increasing hitcount when a correct prediction is made\n",
        "      labelscount=-1\n",
        "      for i in log_probs:\n",
        "        count+=1        \n",
        "        labelscount+=1\n",
        "        if i <0.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(0.0)):\n",
        "            hitcount=hitcount+1\n",
        "        elif i>=.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(1.0)):\n",
        "            hitcount=hitcount+1\n",
        "\n",
        "      # 2. Calculate loss\n",
        "      labels = labels.unsqueeze(1)\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "    print('accuracy:', hitcount/count)\n",
        "\n",
        "test(testloader)\n",
        "\n",
        "print(\"Testing finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZeExvmf8_L5"
      },
      "source": [
        "# re-training of the already trained classifier model for hindi dataset(2.1) now with bengali dataset\n",
        "# transfer learning\n",
        "\n",
        "def train(dataset):\n",
        "  print(\"Training started\")\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  total_loss = 0\n",
        "  stopping_loss_threshold = 1.0\n",
        "  min_loss = 10000\n",
        "  early_stop = False\n",
        "  epochs_loss = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    print(len(dataset))\n",
        "    running_loss = 0.0\n",
        "    counter = 0\n",
        "    \n",
        "    for i, (inputs, labels) in enumerate(dataset):\n",
        "      counter += 1\n",
        "      \n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "      sentiment.zero_grad()\n",
        "      log_probs = sentiment(inputs)\n",
        "\n",
        "      labels = labels.unsqueeze( 1)\n",
        "      loss = criterion(log_probs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item() \n",
        "      # total_loss += loss.item()\n",
        "\n",
        "    # Early stopping as termination condition\n",
        "    print('epoch: %d, loss: %.3f' %\n",
        "                  (epoch + 1, running_loss/len(dataset)))\n",
        "    epochs_loss.append(running_loss/len(dataset))\n",
        "    if (running_loss/len(dataset)) < min_loss:\n",
        "      min_loss = running_loss/len(dataset)\n",
        "      model_state_to_save = sentiment.state_dict()\n",
        "    else:\n",
        "      print(\"Early Stopped\")\n",
        "      break\n",
        "            \n",
        "  print('Minimum loss after early stopping is:', min_loss)\n",
        "  plt.plot(epochs_loss, label=\"Training Loss\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  #saving the model\n",
        "  PATH = './combine.pt'\n",
        "  torch.save(model_state_to_save, PATH)\n",
        "\n",
        "   \n",
        "train(trainloader)\n",
        "\n",
        "print(\"Training finished\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEhFKl6xCdJV"
      },
      "source": [
        "# testing the retrained classifier model for bengali test dataset\n",
        "def test(dataset):\n",
        "    print(\"Testing started\")\n",
        "    count=0\n",
        "\n",
        "    hitcount=0\n",
        "    for inputs, labels in dataset: \n",
        "\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      \n",
        "      \n",
        "      # 1. Generate predictions\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "      log_probs = sentiment(inputs)\n",
        "\n",
        "      labelscount=-1\n",
        "      for i in log_probs:\n",
        "        count+=1        \n",
        "        labelscount+=1\n",
        "        if i <0.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(0.0)):\n",
        "            hitcount=hitcount+1\n",
        "        elif i>=.5:\n",
        "          if torch.eq(labels[labelscount], torch.tensor(1.0)):\n",
        "            hitcount=hitcount+1\n",
        "\n",
        "      \n",
        "      # Calculate loss\n",
        "      labels = labels.unsqueeze(1)\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "    print('accuracy:', hitcount/count)\n",
        "\n",
        "\n",
        "test(testloader)\n",
        "\n",
        "print(\"Testing finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf2yo1dm3VwD"
      },
      "source": [
        "Thank You. \n",
        "Code for part 3 is in seperate ipynb file"
      ]
    }
  ]
}