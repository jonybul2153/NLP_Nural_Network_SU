{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmSDYHrEePOi"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import string \n",
        "import numpy as np\n",
        "import csv\n",
        "import math\n",
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "7YR9xmHIeTfx",
        "outputId": "e3d9344d-7b04-4f94-e8e9-cab552131667"
      },
      "source": [
        "#TODO: implement!\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7bba6f43-1e5a-4266-b301-4cd657e6709f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7bba6f43-1e5a-4266-b301-4cd657e6709f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving hindi_hatespeech.tsv to hindi_hatespeech.tsv\n",
            "Saving model to model\n",
            "Saving stopword.txt to stopword.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBQK0VdEegce"
      },
      "source": [
        "data = pd.read_csv('hindi_hatespeech.tsv', sep='\\t')\n",
        "# data = data.head(100)\n",
        "stop_word_list = pd.read_csv('stopword.txt', sep='\\s+', header=None)\n",
        "stop_word_list = stop_word_list[0].tolist()\n",
        "data['text'] = data['text'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "data['text'] = data['text'].str.replace('[{}]'.format('।'), '')\n",
        "# data['text'] = data['text'].str.replace('[{}]'.format('\\n'), '')\n",
        "data['text'] = data['text'].str.lower()\n",
        "data['text'] = data['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in (stop_word_list)]))\n",
        "data['task_1'] = data['task_1'].map({'HOF': 1, 'NOT': 0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q9g87AteruE",
        "outputId": "f06920f7-494a-4052-ddf9-d723fa26b9c6"
      },
      "source": [
        "#TODO: implement!\n",
        "V = list(data['text'].str.split(' ', expand=True).stack().unique())\n",
        "print(len(V))\n",
        "def word_to_one_hot(word, vocab):\n",
        "  return vocab[word]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7raQywrtr4FD"
      },
      "source": [
        "df = pd.DataFrame(list(zip(V)), columns=['Vocalbulary'])\n",
        "one_hot_vocabulary = pd.get_dummies(df.Vocalbulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBQQY3dIeuxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c131661-7e8c-4a3d-c21e-8264b78a5a09"
      },
      "source": [
        "for each in range(len(data['text'])):\n",
        "  if len(data['text'][each].split(\" \"))<= 25:\n",
        "    for i in range(25 - len(data['text'][each].split(\" \"))):\n",
        "      data['text'][each] = data['text'][each] + \" 0\"\n",
        "  elif len(data['text'][each].split(\" \"))> 25:\n",
        "    x = data['text'][each].split(\" \")[:25]\n",
        "    data['text'][each] = \" \".join(x)\n",
        "  else:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O0qtFwviPNX"
      },
      "source": [
        "# Create model \n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, length, embedding_size):\n",
        "    super().__init__()\n",
        "    # print(\"First.\")\n",
        "    self.input_layer = nn.Linear(in_features=length, out_features=embedding_size, bias=False)\n",
        "    # print(\"Second.\")\n",
        "    self.output = nn.Linear(in_features=embedding_size, out_features=length, bias=False)\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "    x=self.input_layer(one_hot)\n",
        "    x2 = self.output(x)\n",
        "    out = F.log_softmax(x2)\n",
        "    return out\n",
        "\n",
        "# torch.cuda.empty_cache()\n",
        "# model = Word2Vec(len(word_probability_dictionary), embedding_size)\n",
        "# if torch.cuda.is_available():\n",
        "#     model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI8kq9KGeyfR",
        "outputId": "329ba5a3-765d-4ee9-a294-244256fb31d3"
      },
      "source": [
        "# Previous model load.\n",
        "model = Word2Vec(20150, 300)\n",
        "# model.load_state_dict(torch.load(\"/content/model\"))\n",
        "model.load_state_dict(torch.load(\"/content/model\", map_location=torch.device('cpu')))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7qm3DCYSdcv",
        "outputId": "002109a0-cf09-4134-ccfa-14c8ed8750d1"
      },
      "source": [
        "print(model.input_layer.weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.4633, -1.0554, -1.0619,  ...,  0.5338, -0.5103,  0.5778],\n",
            "        [-0.4655, -1.0529, -1.0604,  ...,  0.5328,  0.5096,  0.5848],\n",
            "        [-0.4710, -1.0515, -1.0609,  ..., -0.5349, -0.5120, -0.5819],\n",
            "        ...,\n",
            "        [ 0.4599, -1.0524, -1.0631,  ...,  0.5267,  0.5159,  0.5798],\n",
            "        [-0.4655,  1.0513,  1.0555,  ...,  0.5314,  0.5080,  0.5894],\n",
            "        [-0.4622,  1.0473, -1.0519,  ..., -0.5321, -0.5053, -0.5776]],\n",
            "       requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smJoLD-pgTSF"
      },
      "source": [
        "# # Set hyperparameters\n",
        "window_size = 4\n",
        "embedding_size = 300\n",
        "# More hyperparameters\n",
        "learning_rate = 0.001\n",
        "epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGybpAtsfo38"
      },
      "source": [
        "class sample_data():\n",
        "    \"\"\"Face Landmarks dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,data):\n",
        "      self.data = data\n",
        "    def __len__(self):\n",
        "       return len(self.data)        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "    #   word_to_one_hot_list_for_tensor = []\n",
        "    #   for each in self.data['text'][idx].split(' '):\n",
        "    #     word_to_one_hot_list_for_tensor.append(word_to_one_hot(each,one_hot_vocabulary))\n",
        "    #   # print(\"Print:\",len([self.data['task_1'][idx]]))\n",
        "        # def __getitem__(self, idx):\n",
        "      # word_to_one_hot_list_for_tensor = []\n",
        "      # for each in self.data['text'][idx].split(' '):\n",
        "      #   word_to_one_hot_list_for_tensor.append(word_to_one_hot(each,one_hot_vocabulary))\n",
        "      # print(\"Print:\",len([self.data['task_1'][idx]]))\n",
        "      return self.data['text'][idx], self.data['task_1'][idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    word_to_one_hot_list_for_tensor = []\n",
        "    for _data, _label in batch:\n",
        "      for each in _data.split(' '):\n",
        "        word_to_one_hot_list_for_tensor.append(word_to_one_hot(each,one_hot_vocabulary))\n",
        "    return torch.Tensor(word_to_one_hot_list_for_tensor), torch.Tensor(_label)\n",
        "\n",
        "\n",
        "      # return torch.Tensor(word_to_one_hot_list_for_tensor), torch.Tensor(self.data['task_1'][idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0gXuOo1uaax"
      },
      "source": [
        "sample_custom_dataset = sample_data(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zc_9r3dtlDx"
      },
      "source": [
        "# DataLoader\n",
        "def dataloader_for_model():\n",
        "  # samle_custom_dataset = custom_dataset(data)\n",
        "  # one_hot_tensor = torch.FloatTensor(full_one_hot_list)\n",
        "  # context_tensor = torch.FloatTensor(binary_list)\n",
        "  # # one_hot_tensor = torch.LongTensor(full_one_hot_list)\n",
        "  # # context_tensor = torch.LongTensor(binary_list)\n",
        "  # dataset = TensorDataset(samle_custom_dataset)\n",
        "\n",
        "  loader = DataLoader(\n",
        "      sample_custom_dataset,\n",
        "      batch_size=32,\n",
        "      num_workers=0,\n",
        "      collate_fn = collate_fn,\n",
        "      shuffle=True\n",
        "  )\n",
        "  return loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e8LlARqYnJy"
      },
      "source": [
        "len(V)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EnqIgMngezI"
      },
      "source": [
        "# Create model \n",
        "\n",
        "class Sentiment_Analysis(nn.Module):\n",
        "  def __init__(self, length, embedding_size,model):\n",
        "    super().__init__()\n",
        "    # print(\"First.\")\n",
        "    self.input_layer = nn.Linear(in_features=length, out_features=20150, bias=False)\n",
        "    # print(model.input_layer.weight.shape)\n",
        "    self.fc1 = model.input_layer.weight\n",
        "    self.fc2 = model.output.weight\n",
        "    # print(\"Second.\")\n",
        "    self.pre_out = nn.Linear(in_features=20150, out_features=embedding_size, bias=False)\n",
        "    \n",
        "    self.final = nn.Linear(in_features=embedding_size, out_features=1, bias = False)\n",
        "\n",
        "\n",
        "  def forward(self, one_hot):\n",
        "    x=self.input_layer(one_hot)\n",
        "    print(\" x shape \",x.size())\n",
        "    x = x.view(x.size(0),-1)\n",
        "    # rel = nn.ReLU(x)\n",
        "    x2 = torch.matmul(self.fc1, x)\n",
        "    # print(x2.shape)\n",
        "    x3 = torch.matmul(self.fc2, x2)\n",
        "    # print(x3.shape)\n",
        "\n",
        "    x4 = self.pre_out(x3)\n",
        "    x5 = self.final(x4)\n",
        "    # print(\" x2 shape \",x2.size())\n",
        "    # out = nn.LogSigmoid(x2)\n",
        "    out = F.log_softmax(x5)\n",
        "    return out\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "sentiment = Sentiment_Analysis(len(V), embedding_size,model)\n",
        "torch.cuda.empty_cache()\n",
        "if torch.cuda.is_available():\n",
        "    sentiment.cuda()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh89sPaEgiwl"
      },
      "source": [
        "# Define optimizer and loss\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = optim.Adam(sentiment.parameters(), lr=learning_rate)\n",
        "# optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "torch.cuda.empty_cache()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.BCELoss()\n",
        "# criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "_YMAWjiCgmfp",
        "outputId": "4a633bcc-6ec4-4763-82ed-e6483c0601a9"
      },
      "source": [
        "# # Define train procedure\n",
        "# losses = []\n",
        "\n",
        "# # load initial weights\n",
        "# torch.cuda.empty_cache()\n",
        "def train(dataset):\n",
        "  print(\"Training started\")\n",
        "  # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  total_loss = 0\n",
        "  stopping_loss_threshold = 1.0\n",
        "  early_stop = False\n",
        "  for epoch in range(epochs):\n",
        "    # print(len(dataset))\n",
        "    counter = 0\n",
        "    for data in dataset:\n",
        "      counter += 1\n",
        "      inputs, labels = data\n",
        "\n",
        "      # inputs,labels=inputs.type(torch.int),labels.type(torch.int)\n",
        "      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      # labels = labels.unsqueeze(1)\n",
        "      # print(labels.shape)\n",
        "      # print(inputs.shape)\n",
        "\n",
        "\n",
        "\n",
        "      sentiment.zero_grad()\n",
        "      # inputs = torch.tensor(inputs).to(torch.long)\n",
        "      log_probs = sentiment(inputs)\n",
        "      # print(len(log_probs[0][0]))\n",
        "      # loss = criterion(log_probs, torch.max(labels, 1)[1])\n",
        "      loss = criterion(log_probs, labels)\n",
        "\n",
        "      # loss = criterion(log_probs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "      if counter == 10:\n",
        "        print('epoch:%d, batch:%d, loss: %.3f' %\n",
        "          (epoch + 1, counter, total_loss / 10))\n",
        "        total_loss = 0.0\n",
        "\n",
        "      # if (total_loss / 100) <=stopping_loss_threshold:\n",
        "      #   print(\"Early Stop.\")\n",
        "      #   print('epoch:%d, loss: %.3f' %\n",
        "      #       (epoch + 1, total_loss / 100))\n",
        "      #   early_stop = True\n",
        "      #   break\n",
        "      # else:\n",
        "      # if i % 100 == 99:    # print every 100 mini-batches\n",
        "      #   print('epoch:%d, batch:%d, loss: %.3f' %\n",
        "      #     (epoch + 1, i + 1, total_loss / 100))\n",
        "      #   # losses.append(total_loss/10)\n",
        "      #   total_loss = 0.0\n",
        "    # total_loss = 0.0\n",
        "\n",
        "    # print(losses) \n",
        "\n",
        "    # if early_stop:\n",
        "    #   print(\"Stopped\")\n",
        "    #   break\n",
        "   \n",
        "train(dataloader_for_model())\n",
        "\n",
        "print(\"Training finished\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training started\n",
            "Yo\n",
            " x shape  torch.Size([800, 20150])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b2c594466ad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m#   break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_for_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training finished\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-b2c594466ad5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0msentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;31m# inputs = torch.tensor(inputs).to(torch.long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0;31m# print(len(log_probs[0][0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;31m# loss = criterion(log_probs, torch.max(labels, 1)[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-94b97c6e3b01>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, one_hot)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# rel = nn.ReLU(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# print(x2.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (300x20150 and 800x20150)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIwV3oeIMmpo"
      },
      "source": [
        "Task 2B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GUQFT5dMpCT"
      },
      "source": [
        "data_bangla = pd.read_csv('bengali_hatespeech.csv', sep=',')\n",
        "# data_bangla = data.head(100)\n",
        "bengali_stop_word_list = pd.read_csv('banglastopword.txt', sep='\\s+', header=None)\n",
        "bengali_stop_word_list = bengali_stop_word_list[0].tolist()\n",
        "data_bangla['sentence'] = data_bangla['sentence'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "data_bangla['sentence'] = data_bangla['sentence'].str.replace('[{}]'.format('।'), '')\n",
        "data_bangla['sentence'] = data_bangla['sentence'].str.lower()\n",
        "data_bangla['sentence'] = data_bangla['sentence'].apply(lambda x: ' '.join([item for item in x.split() if item not in (bengali_stop_word_list)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_zEgy0-Mu-l"
      },
      "source": [
        "# Bengali Unique word list\n",
        "bengali_V = list(data_bangla['sentence'].str.split(' ', expand=True).stack().unique())\n",
        "print(len(bengali_V))\n",
        "def word_to_one_hot(word, vocab):\n",
        "  return vocab[word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHpdyXB9Mvt3"
      },
      "source": [
        "bengali_all_word = all_word_corpus(data_bangla['sentence'])\n",
        "len(bengali_all_word)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}