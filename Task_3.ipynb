{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkShaeMfSOdY"
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import os \n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctm-kGToSaeP"
      },
      "source": [
        "#dataset upload\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrBhkclESVPH"
      },
      "source": [
        "Hindi Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdLa_5t1SuAt"
      },
      "source": [
        "# Read Hinid dataset and some preprocessing\n",
        "# Read Hindi Dataset\n",
        "hindi_data = pd.read_csv('hindi_hatespeech.tsv', sep='\\t')\n",
        "hindi_data = hindi_data.loc[:,\"text\":\"task_1\"]\n",
        "hindi_data.dropna(subset = [\"text\"], inplace=True)\n",
        "hindi_data.dropna(subset = [\"task_1\"], inplace=True)\n",
        "hindi_data['task_1'] = hindi_data['task_1'].map({'HOF': 1, 'NOT': 0})\n",
        "hindi_data['text'] = hindi_data['text'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
        "hindi_data['text'] = hindi_data['text'].str.replace('[{}]'.format('ред'), '')\n",
        "hindi_stop_word_list = pd.read_csv('stopword.txt', sep='\\s+', header=None)\n",
        "hindi_stop_word_list = hindi_stop_word_list[0].tolist()\n",
        "hindi_data['text'] = hindi_data['text'].apply(lambda x: ' '.join([item for item in x.split() if item not in (hindi_stop_word_list)]))\n",
        "hindi_training_data = hindi_data.sample(frac = 0.8)\n",
        "hindi_test_data = hindi_data.drop(hindi_training_data.index)\n",
        "# write a dataframe to tsv file\n",
        "hindi_training_data.to_csv(\"hindi_training_data.tsv\", sep=\"\\t\", index=False)\n",
        "# write a dataframe to tsv file\n",
        "hindi_test_data.to_csv(\"hindi_test_data.tsv\", sep=\"\\t\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29bfo5ZCS0k1"
      },
      "source": [
        "#Hindi Dataset Cleanup\n",
        "def hindi_data_cleanup(hindi_data):\n",
        "  cleaned_text = []\n",
        "  for text in hindi_data:\n",
        "      # remove multiple spaces\n",
        "      text = re.sub(r' +', ' ', text)\n",
        "      # remove newline\n",
        "      text = re.sub(r'\\n', ' ', text)\n",
        "      cleaned_text.append(text)\n",
        "  return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXY0yPs2S3S4"
      },
      "source": [
        "#For Creating Train and Validation data\n",
        "char_based = False\n",
        "if char_based:\n",
        "    tokenizer = lambda s: list(s) # char-based\n",
        "else:\n",
        "    tokenizer = lambda s: s.split() # word-based\n",
        "\n",
        "Text = data.Field(preprocessing=hindi_data_cleanup, tokenize=tokenizer, batch_first=True, include_lengths=True, fix_length=100)\n",
        "Label = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\n",
        "\n",
        "fields = [('text', Text), ('labels', Label)]\n",
        "train_data_hindi, test_data_hindi = data.TabularDataset.splits(\n",
        "    path = \"/content\",\n",
        "    train = \"hindi_training_data.tsv\",\n",
        "    test = \"hindi_test_data.tsv\",\n",
        "    format='tsv',\n",
        "    fields=fields,\n",
        "    skip_header=True\n",
        ")\n",
        "\n",
        "seed = 42\n",
        "train_data_hindi, valid_data_hindi = train_data_hindi.split(split_ratio=0.8, random_state=random.seed(seed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8fP2pRmTCjN"
      },
      "source": [
        "def create_iterator(train_data, valid_data, test_data, batch_size, device):    \n",
        "  train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train_data, valid_data, test_data),\n",
        "          batch_size = batch_size,\n",
        "          sort_key = lambda x: len(x.text), \n",
        "          sort_within_batch = True,\n",
        "          device = device)\n",
        "  return train_iterator, valid_iterator, test_iterator\n",
        "\n",
        "def accuracy(probs, target):\n",
        "  predictions = probs.argmax(dim=1)\n",
        "  corrects = (predictions == target)\n",
        "  accuracy = corrects.sum().float() / float(target.size(0))\n",
        "  return accuracy\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.text\n",
        "        text, text_lengths = text.to(device), text_lengths.to(device)\n",
        "        predictions = model(text, text_lengths)\n",
        "        loss = criterion(predictions, batch.labels.squeeze())\n",
        "        acc = accuracy(predictions, batch.labels)        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths).squeeze(1)\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = accuracy(predictions, batch.labels)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "def run_train(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type):\n",
        "  best_valid_loss = float('inf')\n",
        "  training_loss_list = []\n",
        "  valid_loss_list = []\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "      # train the model\n",
        "      train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "\n",
        "      # evaluate the model\n",
        "      valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "      # save the best model\n",
        "      if valid_loss < best_valid_loss:\n",
        "          best_valid_loss = valid_loss\n",
        "          torch.save(model.state_dict(), 'saved_weights'+'_'+model_type+'.pt')\n",
        "      \n",
        "      print(f'Epoch: {epoch+1} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
        "      print(f'Epoch: {epoch+1} | Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXV0WIShTG8l"
      },
      "source": [
        "# Hyperparameters\n",
        "lr = 1e-4\n",
        "batch_size = 50\n",
        "dropout_keep_prob = 0.5\n",
        "embedding_size = 300\n",
        "max_document_length = 100  # each sentence has until 100 words\n",
        "dev_size = 0.8 # split percentage to train\\validation data\n",
        "seed = 1\n",
        "num_classes = 2\n",
        "num_hidden_nodes = 93\n",
        "hidden_dim2 = 128\n",
        "num_layers = 2  # LSTM layers\n",
        "bi_directional = False \n",
        "num_epochs = 7\n",
        "num_hidden_nodes = 93"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Ci2UxFUM86"
      },
      "source": [
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, output_dim, n_layers,\n",
        "                 bidirectional, dropout, pad_index):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                            hidden_dim1,\n",
        "                            num_layers=n_layers,\n",
        "                            bidirectional=bidirectional,\n",
        "                            batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_dim1 * 2, hidden_dim2)\n",
        "        self.fc2 = nn.Linear(hidden_dim2, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.embedding(text)\n",
        "        packed_embedded = pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True) \n",
        "\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        cat = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        rel = self.relu(cat)\n",
        "        dense1 = self.fc1(rel)\n",
        "        drop = self.dropout(dense1)\n",
        "        preds = self.fc2(drop)\n",
        "        return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT0uZEPXTdUO",
        "outputId": "a273af10-3eae-4193-ded7-abdd19b20114"
      },
      "source": [
        "# Main Function\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "  path = '/content'\n",
        "  path_data = os.path.join(path, \"data\")\n",
        "  # parameters\n",
        "  model_type = \"LSTM\"\n",
        "  data_type = \"token\" # or: \"morph\"\n",
        "\n",
        "  char_based = True\n",
        "  if char_based:\n",
        "      tokenizer = lambda s: list(s) # char-based\n",
        "  else:\n",
        "      tokenizer = lambda s: s.split() # word-based\n",
        "\n",
        "  Text.build_vocab(train_data_hindi)\n",
        "  Label.build_vocab(train_data_hindi)\n",
        "  vocab_size = len(Text.vocab)\n",
        "  to_train = True\n",
        "  pad_index = Text.vocab.stoi[Text.pad_token]\n",
        "\n",
        "\n",
        "  train_iterator, valid_iterator, test_iterator = create_iterator(train_data_hindi, valid_data_hindi, test_data_hindi, batch_size, device)\n",
        "\n",
        "  # loss function\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  loss_func = loss_func.to(device)\n",
        "  lstm_model = LSTM(vocab_size, embedding_size, num_hidden_nodes, hidden_dim2 , num_classes, num_layers, bi_directional, dropout_keep_prob, pad_index)\n",
        "  if torch.cuda.is_available():\n",
        "    lstm_model.cuda()\n",
        "\n",
        "  # optimization algorithm\n",
        "  optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
        "\n",
        "  torch.backends.cudnn.enabled = False\n",
        "  run_train(num_epochs, lstm_model, train_iterator, valid_iterator, optimizer, loss_func, model_type)\n",
        "\n",
        "  test_loss, test_acc = evaluate(lstm_model, test_iterator, loss_func)\n",
        "  print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Train Loss: 0.694 | Train Acc: 50.53%\n",
            "Epoch: 1 | Val. Loss: 0.689 |  Val. Acc: 56.17%\n",
            "Epoch: 2 | Train Loss: 0.681 | Train Acc: 59.28%\n",
            "Epoch: 2 | Val. Loss: 0.683 |  Val. Acc: 57.34%\n",
            "Epoch: 3 | Train Loss: 0.666 | Train Acc: 62.83%\n",
            "Epoch: 3 | Val. Loss: 0.670 |  Val. Acc: 58.81%\n",
            "Epoch: 4 | Train Loss: 0.630 | Train Acc: 65.23%\n",
            "Epoch: 4 | Val. Loss: 0.626 |  Val. Acc: 65.36%\n",
            "Epoch: 5 | Train Loss: 0.542 | Train Acc: 72.41%\n",
            "Epoch: 5 | Val. Loss: 0.571 |  Val. Acc: 69.25%\n",
            "Epoch: 6 | Train Loss: 0.455 | Train Acc: 79.38%\n",
            "Epoch: 6 | Val. Loss: 0.543 |  Val. Acc: 72.09%\n",
            "Epoch: 7 | Train Loss: 0.384 | Train Acc: 83.49%\n",
            "Epoch: 7 | Val. Loss: 0.540 |  Val. Acc: 74.90%\n",
            "Test Loss: 0.516 | Test Acc: 77.72%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gU6o1Mw6UhKe"
      },
      "source": [
        "Bangla Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwtoHKHOUfu8"
      },
      "source": [
        "# Read Bangla Dataset\n",
        "bengali_data = pd.read_csv('bengali_hatespeech.csv', sep=',')\n",
        "bengali_data = bengali_data.loc[:,\"sentence\":\"hate\"]\n",
        "bengali_data['sentence'] = bengali_data['sentence'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
        "bengali_data['sentence'] = bengali_data['sentence'].str.replace('[{}]'.format('ред'), '')\n",
        "bengali_stop_word_list = pd.read_csv('banglastopword.txt', sep='\\s+', header=None)\n",
        "bengali_stop_word_list = bengali_stop_word_list[0].tolist()\n",
        "bengali_data['sentence'] = bengali_data['sentence'].apply(lambda x: ' '.join([item for item in x.split() if item not in (bengali_stop_word_list)]))\n",
        "bengali_data = bengali_data[bengali_data.sentence != \"\"]\n",
        "bengali_data.dropna(subset = [\"sentence\"], inplace=True)\n",
        "bengali_data.dropna(subset = [\"hate\"], inplace=True)\n",
        "bengali_training_data = bengali_data.sample(frac = 0.8)\n",
        "bengali_test_data = bengali_data.drop(bengali_training_data.index)\n",
        "# write a dataframe to tsv file\n",
        "bengali_training_data.to_csv(\"bengali_training_data.tsv\", sep=\"\\t\", index=False)\n",
        "# write a dataframe to tsv file\n",
        "bengali_test_data.to_csv(\"bengali_test_data.tsv\", sep=\"\\t\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02bYb1o9iyS_"
      },
      "source": [
        "# This cell is for keeping the size of both dataset equal. If you want the dataset equal, don't run this cell.\n",
        "# bengali_data = bengali_data.sample(n=len(hindi_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wsrxox-V8-Y"
      },
      "source": [
        "#Bengali Dataset Cleanup\n",
        "def bengali_data_cleanup(bengali_data):\n",
        "  cleaned_text = []\n",
        "  for text in bengali_data:\n",
        "      # remove multiple spaces\n",
        "      text = re.sub(r' +', ' ', text)\n",
        "      # remove newline\n",
        "      text = re.sub(r'\\n', ' ', text)\n",
        "      cleaned_text.append(text)\n",
        "  return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdcSu6I3XCvD"
      },
      "source": [
        "Text = data.Field(preprocessing=bengali_data_cleanup, tokenize=tokenizer, batch_first=True, include_lengths=True, fix_length=100)\n",
        "Label = data.Field(sequential=False, use_vocab=False, pad_token=None, unk_token=None)\n",
        "\n",
        "fields = [('text', Text), ('labels', Label)]\n",
        "train_data_bengali, test_data_bengali = data.TabularDataset.splits(\n",
        "    path = \"/content\",\n",
        "    train = \"bengali_training_data.tsv\",\n",
        "    test = \"bengali_test_data.tsv\",\n",
        "    format='tsv',\n",
        "    fields=fields,\n",
        "    skip_header=True\n",
        ")\n",
        "\n",
        "seed = 42\n",
        "train_data_bengali, valid_data_bengali = train_data_bengali.split(split_ratio=0.8, random_state=random.seed(seed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSkD6HjBXfH3",
        "outputId": "c5749e67-e3b6-4ff7-87c8-70daea4fa20d"
      },
      "source": [
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
        "  path = '/content'\n",
        "  path_data = os.path.join(path, \"data\")\n",
        "  # parameters\n",
        "  model_type = \"LSTM\"\n",
        "  data_type = \"token\" \n",
        "\n",
        "  char_based = True\n",
        "  if char_based:\n",
        "      tokenizer = lambda s: list(s) # char-based\n",
        "  else:\n",
        "      tokenizer = lambda s: s.split() # word-based\n",
        "\n",
        "  Text.build_vocab(train_data_bengali)\n",
        "  Label.build_vocab(train_data_bengali)\n",
        "  vocab_size = len(Text.vocab)\n",
        "  pad_index = Text.vocab.stoi[Text.pad_token]\n",
        "\n",
        "\n",
        "  train_iterator, valid_iterator, test_iterator = create_iterator(train_data_bengali, valid_data_bengali, test_data_bengali, batch_size, device)\n",
        "\n",
        "  # loss function\n",
        "  loss_func = nn.CrossEntropyLoss()\n",
        "  loss_func = loss_func.to(device)\n",
        "  lstm_model = LSTM(vocab_size, embedding_size, num_hidden_nodes, hidden_dim2 , num_classes, num_layers, bi_directional, dropout_keep_prob, pad_index)\n",
        "  if torch.cuda.is_available():\n",
        "    lstm_model.cuda()\n",
        "\n",
        "  # optimization algorithm\n",
        "  optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
        "  # train and evaluation\n",
        "\n",
        "  torch.backends.cudnn.enabled = False\n",
        "  run_train(num_epochs, lstm_model, train_iterator, valid_iterator, optimizer, loss_func, model_type)\n",
        "\n",
        "  # load weights\n",
        "  lstm_model.load_state_dict(torch.load(os.path.join(path, \"saved_weights_LSTM.pt\")))\n",
        "  # predict\n",
        "  test_loss, test_acc = evaluate(lstm_model, test_iterator, loss_func)\n",
        "  print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc * 100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 | Train Loss: 0.616 | Train Acc: 67.74%\n",
            "Epoch: 1 | Val. Loss: 0.544 |  Val. Acc: 73.36%\n",
            "Epoch: 2 | Train Loss: 0.482 | Train Acc: 78.41%\n",
            "Epoch: 2 | Val. Loss: 0.512 |  Val. Acc: 75.43%\n",
            "Epoch: 3 | Train Loss: 0.417 | Train Acc: 82.93%\n",
            "Epoch: 3 | Val. Loss: 0.408 |  Val. Acc: 83.55%\n",
            "Epoch: 4 | Train Loss: 0.401 | Train Acc: 84.44%\n",
            "Epoch: 4 | Val. Loss: 0.399 |  Val. Acc: 84.69%\n",
            "Epoch: 5 | Train Loss: 0.450 | Train Acc: 80.35%\n",
            "Epoch: 5 | Val. Loss: 0.486 |  Val. Acc: 79.29%\n",
            "Epoch: 6 | Train Loss: 0.424 | Train Acc: 82.63%\n",
            "Epoch: 6 | Val. Loss: 0.403 |  Val. Acc: 84.27%\n",
            "Epoch: 7 | Train Loss: 0.371 | Train Acc: 85.51%\n",
            "Epoch: 7 | Val. Loss: 0.382 |  Val. Acc: 85.03%\n",
            "Test Loss: 0.371 | Test Acc: 85.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM_USTyeqq3e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}